# Pretrained Embedding Practice

This repository documents a practice exercise carried out by you and your colleague during a college assignment. In this practice, you explored the process of replacing the embedding of an LSTM model with a pretrained embedding, which is a common technique in natural language processing (NLP) tasks. The objective of this exercise was to gain hands-on experience with incorporating pre-trained embeddings to enhance the performance of deep learning models.

## Introduction

The Pretrained Embedding Practice is a part of your college coursework that focuses on applying NLP techniques to improve the performance of deep learning models, specifically Long Short-Term Memory (LSTM) networks. Pretrained word embeddings are precomputed word representations learned from a vast corpus of text, which can be utilized to provide a better initial representation for words in NLP models. This practice explores the integration of such pretrained embeddings into an LSTM model.

## Contents

The tests code and annotations (in spanish) are in nb03.ipynb.

To run the code and run the Jupyter Notebook, you need to have the following dependencies installed:

    Python 3.x
    Jupyter Notebook
    PyTorch
    TorchText
    TorchInfo
    scikit-learn
    numpy
    matplotlib
    cuda (and it's necessary dependencies)

Contributions
Fell free to open issues or pull requests. Or send me an email to adrianpereramoreno@gmail.com. If you see any mistakes or improvements that could be made.
